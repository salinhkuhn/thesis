\chapter{Related Work }

\section {Related work}
This section introduces related work in the area of verified compilation. We present contributions ranging from verified single optimization passes to end-to-end translation validation tools, as well as fully verified compilers. Additionally, we discuss work on the formalization of LLVM IR semantics, which plays a critical role in enabling formal reasoning about the LLVM ecosystem. 

\textbf{Verified Compilation}
Verified compilation has a broad history, with various projects focusing on applying formal methods to compilers. Two common approaches exist in the space: The first approach focuses on the implementation of new fully verified compilation infrastructures with formally proven correctness guarantees. The second approach emphasizes the verification of existing, mainstream compiler toolchains through the use of translation validation techniques. These techniques aim to determine whether compilation stages are semantics-preserving by analyzing the input and output of the compiler and verifying their semantic equivalence.

A prominent example of the first approach is CompCert by [Leroy et al., to be properly cited], a optimizing compiler for a large subset of the C programming language, ".. which is formally verified, using machine-assisted mathematical proofs, to guarantee the absence of compiler bugs." [https://www.absint.com/factsheets/factsheet_compcert_c_web.pdf] CompCert compiles C programs into machine code for various processor targets, including RISC-V, and provides formal correctnss guarantees for its compilation process. The compiler is implemented in the Coq proof assistant and goes through 8 IR's and 15 compilation passes before outputting assembly code.
[https://compcert.org/man/manual.pdf]This progressiv lowering and exposing a machine-code representation inspired the design of our instruction selection to first lower from LLVM IR to an SSA-style assembly language before generating Risc-V machine code. An additionally  contribution of CompCert relevant to this work is its approach to register allocation. In CompCert, register allocation is performed using an unverified algorithm, followed by verifying whether the result is a valid register assignment. This idea is leveraged in our work. However, in contrast to CompCert, our implementation is developed in Lean. This enables a more flexible and efficient verification process compared to the large manual proof development required in CompCert
[to do: not yet happy, needs to be polished]
[https://dl.acm.org/doi/pdf/10.1145/1538788.1538814]

\textbf{Verified Peephole Rewriting}
Assembly-level transformations are common in compilers and are known to be particularly bug-prone. [quote peek paper] Various efforts have been made to verify and implement local rewrite transformations to ensure their correctness. One effort is Peek, a framework for expressing, verifying, and executing machine code transformations for x86 within CompCert. However Peek requires the peephole rewrite patterns to be linear and adjacent. Our peephole optimizations pass supports the detection and optimization of non-linear instruction sequences interleaved with unrelated instructions. An contribution of Peek compared to this thesis is that Peek explicitly models side effects, while our work focuses on pure rewrites. Similar to Peek, we ensure that applying local rewrites preserves the global meaning of the program.
[to do: ready more hoe peek does it]
Additional work on verifying peephole optimizations was conducted by the Lean-MLIR project [7], which is discussed in detail in a dedicated section. In particular, Lean-MLIR formalized and verified several of the peephole rewrites used in InstCombine, a widely-used LLVM optimization pass.
[https://arxiv.org/pdf/2407.03685,
https://coqpl.cs.washington.edu/wp-content/uploads/2014/12/peek.pdf
]

\textbf{Lean-MLIR}
We highlight the Lean-MLIR project in particular, as this work is directly inspired by it and the accompanying paper “Verifying Peephole Rewrites in SSA Compiler IRs”. Lean-MLIR formalizes a framework for implementing SSA-based compiler IR's, mirroring dialects as in   MLIR directly in the Lean theorem prover. As part of the framework, Lean-MLIR provides verified infrastructure for applying common compiler transformation passes such as dead code  and common subexpression elimination. A strength of Lean-MLIR lies in its implementation in  Lean, which enables the use of dependent types and bitvector proof automation via bv\_decide [to do: reference chaptre]. This significantly reduces the amount of manual effort required to discharge proof obligations. In contrast, projects like CompCert, rely more heavily on manually written proofs. The Lean-MLIR framework was used to reimplement and formally verify test cases from Alive2, which demonstrated the use of Lean-MLIR to real world compiler optimization patterns. At the time of this thesis, Lean-MLIR had been used to formulate and verify rewrites within single IR's. However, Lean-MLIR has not been extended to support cross-dialect transformations or used to model hardware-level assembly IR's. This gave the potential to further develop the work in the direction of cross-dialect lowerings and low-level language optimizations. Inspired by this we extend the application of Lean-MLIR and its verification methodology across multiple layers of a compiler pipeline, by adding a instruction selection pass and machine-level transformations.
[to do: potentially write more about the work done on Alive 2 in Lean-MLIR]

\textbf{Mechanized Language Semantics}
There exists a large literature collection on the formalization of programming languages and ISAs, and using them to reason about the correctness of implementations, e.g.
Despite this, there still exists a lack of usable formal foundations within the LLVM compiler design ecosystem. At the heart of the LLVM framework is its IR, whose intended semantics are documented online but do not provide a formalization of the semantics of its IR instructions. Several research projects have focused on the formalization of LLVM IR and building tooling to prove the correctness of IR optimizations. Especially today, where the LLVM compiler framework is at the center of the compilation of more than 10 programming languages, including Rust, C, and Haskell. Meanwhile, Nuno Lopes et al., with Alive2, have shown that automation within the compiler design space is possible and usable, as seen in the commit logs where Alive2 is a highly referenced tool. A common, accepted formalization of LLVM IR is therefore increasingly important.

Vellvm is one effort to formalize the semantics of LLVM IR and provides a framework for reasoning about LLVM IR programs. Unlike our work, Vellvm is implemented in the Coq theorem prover and uses a definition of refinement that differs from the one used in this thesis. We also do not adopt the semantics formalized in Vellvm because they do not satisfy the monotonicity property required we are convinced should hold.
The ongoing efforts surrounding the formalization of LLVM IR are relevant to this work. In particular, we must carefully consider aspects such as LLVM’s undefined behavior when reasoning about instruction selection and proving the correctness of rewrite transformations. In this thesis, we rely on the LLVM IR semantics provided by the Lean-MLIR project, which serves as the semantic foundation for our verification efforts. Further discussion of the semantic assumptions and their implications follows in Chapter XY.
[to do: add further work on this]
[to do: formally show where Vellvm's semantics fail this].
[to do: extend with a more in depth disccusion on work on the semantics]
[https://dl.acm.org/doi/pdf/10.1145/2103621.2103709]
[TO DO check vellvm refinement statement]

